{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich results\n",
    "Incorpora informações úteis ao rankeamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import ast\n",
    "import math\n",
    "import os\n",
    "import ir_datasets\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from ranx import Qrels\n",
    "import re\n",
    "\n",
    "\n",
    "# ------- SOURCES --------\n",
    "vocabulary_file_source = \"../../aditional_data/word-vocab-small.csv\"\n",
    "emb_file_source = \"../../aditional_data/wiki-news-300d-1M.vec\"\n",
    "idf_file = '../../aditional_data/idfnew.norm.tsv'\n",
    "original_data = \"../../dataset/queries_train_judged_expanded/samples.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DADOS ÚTEIS\n",
    "\n",
    "!mkdir -p ../../aditional_data/\n",
    "\n",
    "#Carrega o modelo\n",
    "if os.path.isfile(emb_file_source) == False:\n",
    "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "    !unzip wiki-news-300d-1M.vec.zip\n",
    "    !rm wiki-news-300d-1M.vec.zip\n",
    "    !mv ./wiki-news-300d-1M.vec ../../aditional_data/\n",
    "model = KeyedVectors.load_word2vec_format(emb_file_source, encoding=\"utf-8\", binary=False) #FastText model\n",
    "\n",
    "#Carrega o vocabulário\n",
    "if os.path.isfile(vocabulary_file_source) == False:\n",
    "    !wget https://raw.githubusercontent.com/microsoft/MSMARCO-Passage-Ranking/refs/heads/master/Baselines/data/word-vocab-small.tsv -P ../../aditional_data/\n",
    "vocabulary_pd = pd.read_csv(vocabulary_file_source, names=[\"word\"], sep=\"\\t\")\n",
    "vocabulary = list(set([str(word).lower() for word in vocabulary_pd[\"word\"].to_list()]))\n",
    "\n",
    "# IDF das palavras - o arquivo contém apenas termos de consultas em conjuntos de treinamento/dev/eval do MSMARCO.\n",
    "# Fonte: https://github.com/microsoft/MSMARCO-Passage-Ranking/tree/master/Baselines\n",
    "if os.path.isfile(idf_file) == False:\n",
    "    !wget https://raw.githubusercontent.com/microsoft/MSMARCO-Passage-Ranking/master/Baselines/data/idfnew.norm.tsv -P ../../aditional_data/\n",
    "\n",
    "\n",
    "# Extract qrels (relevance judgments) from the MS MARCO dev judged dataset\n",
    "#dataset = ir_datasets.load(\"msmarco-passage/train/judged\")\n",
    "#qrels = []\n",
    "#for qrel in dataset.qrels_iter():\n",
    "#    qrel # namedtuple<query_id, doc_id, relevance, iteration>\n",
    "#    qrels.append({\n",
    "#        'query_idx': qrel.query_id,\n",
    "#        'passage_idx': qrel.doc_id,\n",
    "#        'relevance': qrel.relevance\n",
    "#    })\n",
    "\n",
    "qrels_dict = Qrels.from_ir_datasets(\"msmarco-passage/train/judged\").to_dict()\n",
    "data = [\n",
    "    (query_id, passage_id, relevance)\n",
    "    for query_id, passages in qrels_dict.items()\n",
    "    for passage_id, relevance in passages.items()\n",
    "]\n",
    "qrels_df = pd.DataFrame(data, columns=[\"query_idx\", \"passage_idx\", \"relevance\"])\n",
    "\n",
    "# Agrupa julgamentos por id da query julgada para contar número de julghamentos\n",
    "relevant_counts = qrels_df.groupby('query_idx').size().reset_index(name=\"relevant_count\").astype('int64') \n",
    "\n",
    "'''\n",
    "    |query_idx   |relevant_count    |\n",
    "    |3           |1                 |\n",
    "'''\n",
    "## Leva até 3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = []\n",
    "new_vocabulary = []\n",
    "for word in vocabulary:\n",
    "    if word in model:\n",
    "        word_vectors.append(np.asarray(model[word], dtype=np.float32))\n",
    "        new_vocabulary.append(word)\n",
    "\n",
    "vocabulary = new_vocabulary.copy()\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "word_vectors.shape\n",
    "## Cada palavra no embedding fasttext tem 300 dimensoes\n",
    "## Agora basta calcular a distancia de cosseno de um embedding para o outro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNÇÕES ÙTEIS\n",
    "\n",
    "def fix_text(text):\n",
    "    text = text.strip()             # Remove espaços iniciais e finais\n",
    "    text = re.sub(' +', ' ', text)  # Remove excesso de espaços\n",
    "    text = text.replace(\" 's\", \"'s\")\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" / \", \"/\")\n",
    "    text = text.replace(\" ?\", \"?\")\n",
    "    return text\n",
    "\n",
    "def calc_spearman(dict1: str, dict2: str, K: int) -> float:\n",
    "    \"\"\"\n",
    "    Example input dictionaries:\n",
    "    dict1 = {'passage1': 1, 'passage2': 2, 'passage3': 3}\n",
    "    dict2 = {'passage1': 3, 'passage2': 2, 'passage3': 1}\n",
    "    \"\"\"\n",
    "\n",
    "    dict1 = ast.literal_eval(dict1)\n",
    "    dict2 = ast.literal_eval(dict2)\n",
    "\n",
    "    # Encontrar as chaves comuns\n",
    "    common_keys = set(dict1.keys()).intersection(set(dict2.keys()))\n",
    "\n",
    "    # Extrair os ranks para as chaves comuns\n",
    "    common_ranks1 = {key: dict1[key] for key in common_keys}\n",
    "    \n",
    "    # Ordenar as chaves comuns de acordo com os valores em dict1 (ordem descendente) e pegar os primeiros K\n",
    "    top_k_keys = sorted(common_ranks1, key=common_ranks1.get, reverse=True)[:K]\n",
    "\n",
    "    # Extrair os ranks para as chaves comuns filtradas pelo top K\n",
    "    ranks1 = [dict1[key] for key in top_k_keys]\n",
    "    ranks2 = [dict2[key] for key in top_k_keys]\n",
    "\n",
    "    # Calcular a correlação de Spearman\n",
    "    spearman_corr, _ = spearmanr(ranks1, ranks2)\n",
    "\n",
    "    return spearman_corr\n",
    "\n",
    "\n",
    "def calc_avg_precision(dict: str, K: int) -> float:\n",
    "    \"\"\"\n",
    "    Example input dictionary:\n",
    "    dict = {'passage1': 1, 'passage2': 2, 'passage3': 3}\n",
    "    \"\"\"\n",
    "    \n",
    "    dict = ast.literal_eval(dict)\n",
    "\n",
    "    # Pegar os primeiros K elementos com base no rank (ordem descendente)\n",
    "    top_k_keys = sorted(dict, key=dict.get, reverse=True)[:K]\n",
    "\n",
    "    # Calcular a média de precisão\n",
    "    ranks = [dict[key] for key in top_k_keys]\n",
    "    avg_precision = sum(ranks) / len(ranks) if ranks else 0.0\n",
    "\n",
    "    return avg_precision\n",
    "\n",
    "def similarity(originalPhrase: str, expandedPhrase: str, word_vectors):\n",
    "    '''\n",
    "    Calcula a similaridade entre as palavras diferentes de duas frases.\n",
    "    Se mais de uma palavra for diferente, o resultado será a média das similaridades.\n",
    "    '''\n",
    "    similarities = []\n",
    "    originalPhraseWords = originalPhrase.split()\n",
    "    expandedPhraseWords = expandedPhrase.split()\n",
    "    \n",
    "    for wordIndex in range(0, len(originalPhraseWords)):\n",
    "        originalWord = originalPhraseWords[wordIndex]\n",
    "        expandedWord = expandedPhraseWords[wordIndex]\n",
    "\n",
    "        if(originalWord == expandedWord):\n",
    "            continue\n",
    "\n",
    "        emb1 = word_vectors[vocabulary.index(originalWord)]\n",
    "        emb2 = word_vectors[vocabulary.index(expandedWord)]\n",
    "\n",
    "        similarities.extend(cosine_similarity([emb1], [emb2]))\n",
    "     \n",
    "    return(sum(similarities)/len(similarities))\n",
    "\n",
    "def idf(originalPhrase: str, expandedPhrase: str, idfListOriginal: list, idfListExpanded: list):\n",
    "    '''\n",
    "        Recupera o idf das palavras diferentes de duas frases, além da diferença entre o idf da frase original e da expandida.\n",
    "        Se mais de uma palavra for diferente, o resultado será a média dos idfs.\n",
    "    '''    \n",
    "\n",
    "    if len(idfListOriginal) != len(originalPhrase.split()) or len(idfListExpanded) != len(expandedPhrase.split()):\n",
    "        raise ValueError(\"idfListOriginal and idfListExpanded must be the same length as originalPhrase and expandedPhrase\")\n",
    "\n",
    "    \n",
    "    expanded_idfs = []\n",
    "    difference_idfs = []\n",
    "\n",
    "    originalPhraseWords = originalPhrase.split()\n",
    "    expandedPhraseWords = expandedPhrase.split()\n",
    "\n",
    "    for wordIndex in range(0, len(originalPhraseWords)):\n",
    "        originalWord = originalPhraseWords[wordIndex]\n",
    "        expandedWord = expandedPhraseWords[wordIndex]\n",
    "\n",
    "        if(originalWord == expandedWord):\n",
    "            continue\n",
    "\n",
    "\n",
    "        if idfListOriginal[wordIndex] is None or idfListExpanded[wordIndex] is None:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "        \n",
    "        expanded_idfs.append(idfListExpanded[wordIndex])\n",
    "        difference_idfs.append(idfListOriginal[wordIndex]-idfListExpanded[wordIndex])\n",
    "        \n",
    "\n",
    "    mean_expanded_idfs = sum(expanded_idfs)/len(expanded_idfs)\n",
    "    mean_of_diference_idfs = sum(difference_idfs)/len(difference_idfs)\n",
    "    \n",
    "    return mean_expanded_idfs, mean_of_diference_idfs\n",
    "\n",
    "# Função para tokenizar documentos\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Função para retornar o vetor TF-IDF de uma sentença\n",
    "def get_sentence_tfidf(sentence, idf):\n",
    "    tokenized_sentence = tokenize(sentence)\n",
    "    sentence_tf = Counter(tokenized_sentence)\n",
    "    sentence_tfidf = {term: (sentence_tf[term] / sum(sentence_tf.values())) * idf.get(term, 0) for term in tokenized_sentence}\n",
    "    return sentence_tfidf\n",
    "\n",
    "# Função para retornar o IDF de cada termo em uma sentença\n",
    "def get_sentence_idf(sentence, idf):\n",
    "    tokenized_sentence = tokenize(sentence)\n",
    "    sentence_idf = {term: idf.get(term, 0) for term in tokenized_sentence}\n",
    "    return sentence_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recupera as informações textuais e identificadores originais das queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries_info_pd = pd.DataFrame(pd.read_pickle(original_data))\n",
    "\n",
    "expanded_queries_info_pd['query_idx'] = expanded_queries_info_pd['query_exp_id'].apply(lambda query: query.split('_')[0]).astype('int64')\n",
    "expanded_queries_info_pd['exp_num'] = expanded_queries_info_pd['query_exp_id'].apply(lambda query: query.split('_')[1]).astype('int64')\n",
    "\n",
    "#expanded_queries_info_pd = expanded_queries_info_pd[['query_idx', 'exp_num', 'query', 'passage_idx', 'passage']]\n",
    "expanded_queries_info_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recupera o rankeamento das queries expandidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona o fold desejado\n",
    "fold = 0\n",
    "\n",
    "expanded_queries_rank_pd =  pd.read_csv(f\"../../ranking/RetrieverBERT_queries_train_judged_expanded/RetrieverBERT_queries_train_judged_expanded_{fold}.rnk.csv\", sep=\"\\t\")     \n",
    "\n",
    "expanded_queries_rank_pd = expanded_queries_rank_pd.sort_values(by=['Query']).drop_duplicates()\n",
    "\n",
    "print(f\"Foram obtidos {len(expanded_queries_rank_pd)} resgistros dos folds. Cada um referente a um par (expansão-rank).\")\n",
    "expanded_queries_rank_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa o número referente ao par\n",
    "expanded_queries_rank_pd['query_run_idx'] = expanded_queries_rank_pd['Query'].apply(lambda query: query.split('_')[1]).astype('int64')  # transformação: query_3 -> 3\n",
    "expanded_queries_docs_rank_pd = expanded_queries_rank_pd[[\"query_run_idx\", \"Passage_Scores\"]]\n",
    "\n",
    "expanded_queries_docs_rank_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera o identificador original das queries\n",
    "expanded_queries_docs_rank_info_pd = expanded_queries_docs_rank_pd.merge(expanded_queries_info_pd, left_on='query_run_idx', right_on='idx', suffixes=('_x', ''))\n",
    "expanded_queries_docs_rank_info_pd = expanded_queries_docs_rank_info_pd[['query_idx', 'exp_num','Passage_Scores', 'query_exp_id']]\n",
    "\n",
    "print(f\"São {expanded_queries_docs_rank_info_pd['query_idx'].nunique()} queries originais em jogo\")\n",
    "expanded_queries_docs_rank_info_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifica as queries que deram origem às expansões (aquelas com 'exp_num' igual a 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.max_columns = 500\n",
    "\n",
    "print(f\"Entraram {len(expanded_queries_docs_rank_info_pd)} queries expandidas.\")\n",
    "grouped_expansions = expanded_queries_docs_rank_info_pd.groupby('query_idx')\n",
    "\n",
    "one_expansions = []\n",
    "\n",
    "for query_idx, group in grouped_expansions:\n",
    "    # Filtrar o registro onde 'exp_num' é igual a 1\n",
    "    filtered_rows = group[group['exp_num'] == 1]\n",
    "    one_expansions.append(filtered_rows)\n",
    "\n",
    "# Concatenar todos os DataFrames resultantes em um único DataFrame\n",
    "one_expansions = pd.concat(one_expansions, ignore_index=True)\n",
    "one_expansions = one_expansions.drop_duplicates()\n",
    "\n",
    "#print(f\"{df_first_rows['query_idx'].nunique()} queries originais\")\n",
    "print(len(one_expansions))\n",
    "one_expansions.head(15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega o número de documentos relevantes por query (obtidos da MSMARCO)\n",
    "original_queries = one_expansions.merge(relevant_counts, on=\"query_idx\")\n",
    "original_queries.rename(columns={\"Passage_Scores\":\"original_passage_scores\"}, inplace=True)\n",
    "\n",
    "original_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifica as queries são realmente expansões (aquelas com 'exp_num' diferente de 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries = expanded_queries_docs_rank_info_pd[expanded_queries_docs_rank_info_pd[\"exp_num\"] != 1].copy()\n",
    "expanded_queries.rename(columns={\"Passage_Scores\":\"expansion_passage_scores\"}, inplace=True)\n",
    "\n",
    "expanded_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unifica as queries originais com as expansões\n",
    "E agrega o número de documentos relevantes julgados na MsMarco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanded_queries_ranks_pd  = expanded_queries.copy()\n",
    "\n",
    "\n",
    "expanded_queries_ranks_pd = pd.merge(left=expanded_queries, right=original_queries, on='query_idx')\n",
    "\n",
    "#expanded_queries_ranks_pd[\"original_passage_scores\"] = expanded_queries_ranks_pd.apply(\n",
    "#    lambda row: original_queries.loc[original_queries[\"query_idx\"] == row[\"query_idx\"]][\"original_passage_scores\"].values[0], axis=1\n",
    "#)\n",
    "\n",
    "#expanded_queries_ranks_pd[\"relevant_count\"] = expanded_queries_ranks_pd.apply(\n",
    "#    lambda row: original_queries.loc[original_queries[\"query_idx\"] == row[\"query_idx\"]][\"relevant_count\"].values[0], axis=1\n",
    "#)\n",
    "\n",
    "print(expanded_queries_ranks_pd['original_passage_scores'].nunique())\n",
    "expanded_queries_ranks_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcula o Spearman entre cada registro original e suas expansões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd = expanded_queries_ranks_pd.copy()\n",
    "spearman_pd['spearman'] = spearman_pd.apply(\n",
    "    lambda row: calc_spearman(row['original_passage_scores'], row['expansion_passage_scores'], row['relevant_count']),\n",
    "    axis=1   \n",
    ") \n",
    "\n",
    "#master_info_dd.compute()\n",
    "print(f\"queries originais mantidas até aqui: {spearman_pd['query_idx'].nunique()}\")\n",
    "print(spearman_pd.head(5))\n",
    "print(len(spearman_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcula o a precisão média da query original e das expansões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd['avg_precision_query_original'] = spearman_pd.apply(\n",
    "    lambda row: calc_avg_precision(row['original_passage_scores'], row['relevant_count']),\n",
    "    axis=1   \n",
    ")\n",
    "\n",
    "spearman_pd['avg_precision_query_expansao'] = spearman_pd.apply(\n",
    "    lambda row: calc_avg_precision(row['expansion_passage_scores'], row['relevant_count']),\n",
    "    axis=1   \n",
    ")\n",
    "\n",
    "print(f\"queries originais mantidas até aqui: {spearman_pd['query_idx'].nunique()}\")\n",
    "print(spearman_pd.head(5))\n",
    "print(len(spearman_pd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(spearman_pd['avg_precision_query_original'], bins=15, color='blue', edgecolor='black')\n",
    "\n",
    "# Set the y-axis to log scale\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Precisão')\n",
    "plt.ylabel('Freqência')\n",
    "plt.title('Frequência de faixas de precisão para a consulta original')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtém os textos das queries original e expandida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#spearman_pd.drop(columns=['original_passage_scores', 'expansion_passage_scores'], inplace= True)\n",
    "\n",
    "spearman_pd[\"query_original\"] = spearman_pd.apply(\n",
    "    lambda row: expanded_queries_info_pd.loc[expanded_queries_info_pd[\"query_idx\"] == row[\"query_idx\"], \"query\"].values[0], axis=1\n",
    ")\n",
    "\n",
    "spearman_pd[\"query_expandida\"] = spearman_pd.apply(\n",
    "    lambda row: expanded_queries_info_pd.loc[\n",
    "        (expanded_queries_info_pd[\"query_idx\"] == row[\"query_idx\"]) & \n",
    "        (expanded_queries_info_pd[\"exp_num\"] == row[\"exp_num_x\"]), \n",
    "        \"query\"\n",
    "    ].values[0], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"queries originais mantidas até aqui: {spearman_pd['query_idx'].nunique()}\")\n",
    "print(len(spearman_pd))\n",
    "print(print(spearman_pd.head(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTRA queries sem julgamento suficiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não é necessario, já que essa filtragem foi feita antes de realizar a expansão\n",
    "\n",
    "# #filtered_df = spearman_pd[spearman_pd['relevant_count'] >= 5]\n",
    "#filtered_df.count()\n",
    "#len(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agrega IDF das palavras da query original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_pd = pd.read_csv(\"../../aditional_data/idfnew.norm.tsv\", sep=\"\\t\", header=None, names=[\"word\", \"idf\"])\n",
    "\n",
    "# Create a dictionary from df_idf for quick lookup\n",
    "idf_dict = dict(zip(IDF_pd['word'], IDF_pd['idf']))\n",
    "\n",
    "# Function to map words to their idf values\n",
    "def map_idf(query):\n",
    "    words = query.split()\n",
    "    idf_values = [idf_dict.get(word, None) for word in words]  # Use None if the word is not found\n",
    "    return idf_values\n",
    "\n",
    "# Apply the function to the 'query_original' column\n",
    "spearman_pd['idf_original_values'] = spearman_pd['query_original'].apply(map_idf)\n",
    "spearman_pd['idf_expanded_values'] = spearman_pd['query_expandida'].apply(map_idf)\n",
    "\n",
    "print(f\"queries originais mantidas até aqui: {spearman_pd['query_idx'].nunique()}\")\n",
    "spearman_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria o indicador de similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd[\"words_similarity\"] = spearman_pd.apply(lambda row: similarity(row[\"query_original\"], row[\"query_expandida\"], word_vectors)[0], axis=1)\n",
    "print(f\"queries originais mantidas até aqui: {spearman_pd['query_idx'].nunique()}\")\n",
    "\n",
    "# Demora cerca de 2m para executar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria os indicadores de idf (idf do(s) termo(s) expandido(s) e diferença entre idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd[[\"expansion_idf\", \"expansion_idf_difference\"]] = spearman_pd.apply(\n",
    "    lambda row: idf(row[\"query_original\"], row[\"query_expandida\"], row[\"idf_original_values\"], row[\"idf_expanded_values\"]), \n",
    "    axis=1,\n",
    "    result_type=\"expand\"\n",
    "    )\n",
    "\n",
    "print(f\"queries originais mantidas até aqui: {spearman_pd['query_idx'].nunique()}\")\n",
    "spearman_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria os labels com base na precisão média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se avg_precision_query_expansao >= avg_precision_query_original então label = 1\n",
    "spearman_pd[\"label\"] = spearman_pd.apply(\n",
    "    lambda row: 1 if row[\"avg_precision_query_expansao\"] >= row[\"avg_precision_query_original\"] else 0, axis=1\n",
    "    )\n",
    "\n",
    "print(f\"queries originais mantidas até aqui: {spearman_pd['query_idx'].nunique()}\")\n",
    "spearman_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salva resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd = spearman_pd.rename(columns={'exp_num_x':'exp_num'})\n",
    "spearman_pd.to_csv(f'./queries_train_judged_expanded_enriched.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
