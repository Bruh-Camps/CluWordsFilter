{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from ranx import Run, fuse, evaluate, Qrels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Definições\n",
    "\n",
    "Esta seção define como a fusão é realizada e testa o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funções úteis nessa parte\n",
    "\n",
    "# Corrige as labels de \"x_y\" para \"x\"\n",
    "def correct_scores(documents_dict_str):\n",
    "    documents_dict = ast.literal_eval(documents_dict_str)\n",
    "\n",
    "    new_dict = {}\n",
    "    for k, v in documents_dict.items():\n",
    "        new_key = k.split(\"_\")[1]\n",
    "        new_dict[new_key] = v\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "def filter_good_expansions(full_dataset, model=None):\n",
    "    if model is None:\n",
    "        grouped = full_dataset.groupby('query_idx')\n",
    "        filtered_dataset = []\n",
    "\n",
    "        for _, group in grouped:\n",
    "            # Get the first row's values for specific columns\n",
    "            first_row = group[['query_idx', 'exp_num_y', 'original_passage_scores', 'bm25_scores']].iloc[0].tolist()\n",
    "            filtered_dataset.append(first_row)\n",
    "\n",
    "            # Filter rows where 'label' is 1\n",
    "            ones_label = group[group['label'] == 1]\n",
    "            if not ones_label.empty:  # Only add if there's at least one matching row\n",
    "                filtered_values = ones_label[['query_idx', 'exp_num', 'expansion_passage_scores', 'bm25_scores']].values.tolist()\n",
    "                filtered_dataset.extend(filtered_values)  # Flatten and add to dataset\n",
    "\n",
    "    else:\n",
    "        clean_dataset = full_dataset[[ \"relevant_count\",\"spearman\" , \"words_similarity\",  \"expansion_idf\", \"expansion_idf_difference\"]] \n",
    "        clean_dataset = clean_dataset.rename(columns={\n",
    "                                                        'words_similarity': 'words_semantic_similarity',\n",
    "                                                        'relevant_count': 'k_relevance_judgments',\n",
    "                                                        'spearman':'spearman_rank_correlation'\n",
    "                                                    })\n",
    "\n",
    "        clean_dataset = clean_dataset.apply(pd.to_numeric)\n",
    "        predictions = model.predict(clean_dataset)\n",
    "        model_dataset = full_dataset.copy()\n",
    "        model_dataset[\"predictions\"] = predictions\n",
    "\n",
    "        grouped = model_dataset.groupby('query_idx')\n",
    "        filtered_dataset = []\n",
    "\n",
    "        for _, group in grouped:\n",
    "            # Get the first row's values for specific columns\n",
    "            first_row = group[['query_idx', 'exp_num_y', 'original_passage_scores', 'bm25_scores']].iloc[0].tolist()\n",
    "            filtered_dataset.append(first_row)\n",
    "\n",
    "            # Filter rows where 'label' is 1\n",
    "            ones_prediction = group[group['predictions'] == 1]\n",
    "            if not ones_prediction.empty:  # Only add if there's at least one matching row\n",
    "                filtered_values = ones_prediction[['query_idx', 'exp_num', 'expansion_passage_scores', 'bm25_scores']].values.tolist()\n",
    "                filtered_dataset.extend(filtered_values)  # Flatten and add to dataset\n",
    "\n",
    "    # Create DataFrame from the filtered dataset\n",
    "    columns = ['query_idx', 'exp_num', 'passage_scores', 'bm25_scores']  # Update as per your columns\n",
    "    return pd.DataFrame(filtered_dataset, columns=columns).sort_values('exp_num')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fuse_expansions(dataset_to_fuse:pd.DataFrame, method:str, include_bm25:bool=False):\n",
    "\n",
    "    if include_bm25 and ('bm25_scores' not in dataset_to_fuse.columns):\n",
    "        print(\"A coluna bm25_scores do dataset nao foi encontrada.\")\n",
    "        return\n",
    "    \n",
    "    # Corrige chave das passagens  {'passage_172362': 0.3176,...} --> {'172362': 0.3176,...}\n",
    "    filtered_df = dataset_to_fuse.copy()\n",
    "    filtered_df[\"passage_scores\"] = filtered_df[\"passage_scores\"].apply(lambda documents_dict: correct_scores(documents_dict))\n",
    "    if include_bm25:\n",
    "        filtered_df[\"bm25_scores\"] = filtered_df[\"bm25_scores\"].apply(lambda documents_dict: correct_scores(documents_dict))\n",
    "\n",
    "\n",
    "    grouped_df = filtered_df.groupby('query_idx')\n",
    "\n",
    "    # Inicializa o conjunto total de fusões\n",
    "    all_fused_runs = {}\n",
    "    count = 0\n",
    "    # Como temos expansões de queries distintas, temos que agrupar de acordo com a query de origem antes de fundir.\n",
    "    # Posteriormente elas as fusões de cada agrupamento podem compor um conjunto total das fusões.\n",
    "    for query_idx, group in grouped_df:\n",
    "\n",
    "        count +=1\n",
    "        # Cria as runs (a partir da query original + expansões referentes a ela)\n",
    "        runs = {}\n",
    "        for _, row in group.iterrows():\n",
    "            runs[f\"{query_idx}_exp_{row['exp_num']}\"] = Run({str(query_idx): row[\"passage_scores\"]})\n",
    "\n",
    "        # Se for escolhido agregar os resultados da BM25\n",
    "        if include_bm25:\n",
    "            runs[f\"{query_idx}_bm25_exp_{row['exp_num']}\"] = Run({str(query_idx): row[\"bm25_scores\"]})\n",
    "                \n",
    "        # Faz a fusão a partir dos runs\n",
    "        fused_run = fuse(list(runs.values()), method=method) if len(runs) > 1 else list(runs.values())[0] \n",
    "        all_fused_runs.update(fused_run.to_dict())\n",
    "\n",
    "    #print(f\"tamanho do all_sused_runs {len(all_fused_runs)}, quando deveriam ser {count}\")\n",
    "\n",
    "    fused_runs = Run(all_fused_runs)\n",
    "\n",
    "    #print(f\"Tamanho do dataset de expansões usado: {len(filtered_df)}\")\n",
    "    #print(f\"Número de queries após fusão: {len(fused_runs)}\")\n",
    "    return fused_runs\n",
    "\n",
    "def evaluate_fusion(fused_runs:Run, relevance_map:dict, metrics:list):\n",
    "    qrels_query_ids = set(relevance_map.keys())\n",
    "    run_query_ids = set(fused_runs.keys())\n",
    "\n",
    "    # Identificar diferenças\n",
    "    missing_in_run = qrels_query_ids - run_query_ids\n",
    "    missing_in_qrels = run_query_ids - qrels_query_ids\n",
    "\n",
    "    #print(\"Faltando no Run:\", missing_in_run)\n",
    "    #print(\"Faltando no Qrels:\", missing_in_qrels)\n",
    "\n",
    "    run_dict = fused_runs.to_dict()\n",
    "\n",
    "    qrels_query_ids = set(relevance_map.keys())\n",
    "    run_query_ids = set(run_dict.keys())\n",
    "\n",
    "    # Consultas faltantes no Run\n",
    "    missing_in_run = qrels_query_ids - run_query_ids\n",
    "    for query_id in missing_in_run:\n",
    "        run_dict[query_id] = {}  # Adiciona consulta com resultados vazios (para não criar distorções)\n",
    "\n",
    "    # Consultas extras no Run\n",
    "    missing_in_qrels = run_query_ids - qrels_query_ids\n",
    "    for query_id in missing_in_qrels:\n",
    "        run_dict.pop(query_id)\n",
    "\n",
    "    # Ordenar o Qrels (relevance_map) e os runs\n",
    "    sorted_relevance_map = {query_id: relevance_map[query_id] for query_id in sorted(relevance_map.keys())}\n",
    "    sorted_run_dict = {query_id: run_dict[query_id] for query_id in sorted(run_dict.keys())}\n",
    "    fused_runs = Run(sorted_run_dict)\n",
    "    relevance_map = Qrels(sorted_relevance_map)\n",
    "\n",
    "    # Avalia as fusões de cada grupo de expansões\n",
    "    return evaluate(\n",
    "        sorted_relevance_map, \n",
    "        fused_runs,\n",
    "        metrics=metrics,\n",
    "        make_comparable=True  \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RECUPERA DADOS IMPORTANTES PARA A FUSÃO\n",
    "\n",
    "# DATASET COM AS EXPANSÕES ENRIQUECIDAS\n",
    "full_dataset = pd.read_csv(\n",
    "    \"../1_enrich_results/queries_train_judged_expanded_enriched.csv\",\n",
    "    sep='\\t',\n",
    "    index_col=0\n",
    ")\n",
    "print(f\"queries originais mantidas até aqui: {full_dataset['query_idx'].nunique()}\")\n",
    "\n",
    "# QRELS (relevance judgments) do dataset MS MARCO dev judged\n",
    "qrels = Qrels.from_ir_datasets(\"msmarco-passage/train/judged\")\n",
    "\n",
    "# MODELO ANTERIORMENTE TREINADO\n",
    "model = xgb.XGBClassifier()\n",
    "model.load_model(\"../3_xgboost_model/xgboost_model.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RECONSTRÓI O MAPA DE RELEVÂNCIA\n",
    "# O mapa de relevância foi salvo na etapa \"create_folds\", \n",
    "# Como o mesmo id da query original (query_idx) se repete para muitas expansões, foi criado um idx temporáro para elas\n",
    "# porém, depois que a fusão é realizada, temos que transformar o mapa de relevância para refletir novamente o id original da query e calcular corretamente as métricas\n",
    "\n",
    "\n",
    "with open(f\"../../input_data/samples.pkl\", \"rb\") as dataset_file:\n",
    "    samples = pickle.load(dataset_file)\n",
    "    samples = pd.DataFrame(samples)\n",
    "\n",
    "relevance_map_path = \"../../input_data/relevance_map.pkl\"\n",
    "with open(relevance_map_path, \"rb\") as relevances_file:\n",
    "    data = pickle.load(relevances_file)\n",
    "\n",
    "incorrect_relevance_map = {}\n",
    "for text_idx, labels_ids in data.items():\n",
    "    d = {}\n",
    "    for label_idx in labels_ids:\n",
    "        d[f\"{label_idx}\"] = 1.0\n",
    "    incorrect_relevance_map[f\"{text_idx}\"] = d \n",
    "\n",
    "relevance_map = {}\n",
    "for key, item in incorrect_relevance_map.items():\n",
    "    new_key = samples.loc[samples['idx'] == int(key)]['query_exp_id'].values[0].split('_')[0]\n",
    "    relevance_map[new_key] = item\n",
    "\n",
    "#relevance_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1** Avaliação contendo apenas as consultas originais\n",
    "\n",
    "Apenas os registros que tem exp_num = 1, que são os originais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = full_dataset.groupby('query_idx')\n",
    "filtered_dataset = {}\n",
    "\n",
    "for query_idx, group in grouped:\n",
    "    first_row = group[['query_idx', 'exp_num_y', 'original_passage_scores']].iloc[0]\n",
    "    filtered_dataset[str(first_row['query_idx'])] = correct_scores(first_row['original_passage_scores'])\n",
    "original_runs = Run(filtered_dataset)\n",
    "\n",
    "original_eval_results = evaluate_fusion(original_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "print(\"Combined Evaluation Results:\", original_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2** Fusão contendo todas as expansões consideradas boas\n",
    "\n",
    "Faz o rank fusion usando informação a privilegiada do dataset (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_expansions_ideal = filter_good_expansions(full_dataset)\n",
    "ideal_fused_runs = fuse_expansions(filtered_expansions_ideal, \"mnz\")\n",
    "\n",
    "eval_results = evaluate_fusion(ideal_fused_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "print(\"Combined Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTADO DA FUSÂO\n",
    "ideal_fused_runs_dict = ideal_fused_runs.to_dict()\n",
    "fused_dataset_ideal_filter = full_dataset[[\"query_idx\", \"query_original\"]].copy().drop_duplicates().reset_index(drop=True)\n",
    "fused_dataset_ideal_filter[\"fused_ranking\"] = fused_dataset_ideal_filter[\"query_idx\"].apply(lambda x: ideal_fused_runs_dict[str(x)] if str(x) in ideal_fused_runs_dict.keys() else {})\n",
    "fused_dataset_ideal_filter.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Fusão contendo as expansões boas identificadas pelo modelo anteriormente treinado\n",
    "\n",
    "Faz o rank fusion usando o modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_expansions_model = filter_good_expansions(full_dataset, model=model)\n",
    "model_fused_runs = fuse_expansions(filtered_expansions_model, \"mnz\")\n",
    "eval_results_model = evaluate_fusion(model_fused_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "print(\"Combined Evaluation Results:\", eval_results_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTADO DA FUSÂO\n",
    "model_fused_runs_dict = model_fused_runs.to_dict()\n",
    "fused_dataset_model_filter = full_dataset[[\"query_idx\", \"query_original\"]].copy().drop_duplicates().reset_index(drop=True)\n",
    "fused_dataset_model_filter[\"fused_ranking\"] = fused_dataset_model_filter[\"query_idx\"].apply(lambda x: model_fused_runs_dict[str(x)] if str(x) in model_fused_runs_dict.keys() else {})\n",
    "fused_dataset_model_filter.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrega um novo retriever (BM25) para as expansões consideradas boas pelo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_expansions_model = filter_good_expansions(full_dataset, model=model)\n",
    "model_fused_runs = fuse_expansions(filtered_expansions_model, \"mnz\", True)\n",
    "eval_results_model = evaluate_fusion(model_fused_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "print(\"Combined Evaluation Results:\", eval_results_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Fusão e avaliação dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os algoritmos de fusão utilizados aqui são aqueles que não requerem parâmetro de otimização\n",
    "results={}\n",
    "fusion_algorithms = ['min', 'med', 'anz', 'log_isr', 'bordafuse', 'condorcet', 'max', 'sum', 'mnz', 'isr']\n",
    "\n",
    "for algorithm in fusion_algorithms:\n",
    "\n",
    "    grouped = full_dataset.groupby('query_idx')\n",
    "    filtered_dataset = {}\n",
    "    for query_idx, group in grouped:\n",
    "        first_row = group[['query_idx', 'exp_num_y', 'original_passage_scores']].iloc[0]\n",
    "        filtered_dataset[str(first_row['query_idx'])] = correct_scores(first_row['original_passage_scores'])\n",
    "    original_runs = Run(filtered_dataset)\n",
    "    original_eval_results = evaluate_fusion(original_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "    results['1-orign_' + algorithm] = original_eval_results\n",
    "\n",
    "    ### Faz a expansão com info privilegiada\n",
    "    ideal_filtered_expansions = filter_good_expansions(full_dataset)\n",
    "    ideal_fused_runs = fuse_expansions(ideal_filtered_expansions, algorithm)\n",
    "    ideal_eval_results = evaluate_fusion(ideal_fused_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "    results['4-ideal_' + algorithm] = ideal_eval_results\n",
    "\n",
    "    ### Faz a expansão com julgada pelo modelo\n",
    "    model_filtered_expansions = filter_good_expansions(full_dataset, model=model)\n",
    "    model_fused_runs = fuse_expansions(model_filtered_expansions, algorithm)\n",
    "    model_eval_results_model = evaluate_fusion(model_fused_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "    results['2-model_' + algorithm] = model_eval_results_model\n",
    "\n",
    "    ### Faz a expansão com julgada pelo modelo acrescentando os resultados da bm25\n",
    "    bm25_filtered_expansions = filter_good_expansions(full_dataset, model=model)\n",
    "    bm25_fused_runs = fuse_expansions(bm25_filtered_expansions, \"mnz\", True)\n",
    "    eval_results_bm25 = evaluate_fusion(bm25_fused_runs, relevance_map, [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"])\n",
    "    results['3-model+bm25_' + algorithm] = eval_results_bm25\n",
    "\n",
    "    print(f\"Expansões com {algorithm} realizada com sucesso!\")\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular as variações percentuais em relação à linha \"1-orign\"\n",
    "df_plot = df_results.copy()\n",
    "\n",
    "df_plot['algorithm'] = df_plot.index.str.split('_').str[1]\n",
    "df_plot['prefix'] = df_plot.index.str.split('_').str[0]\n",
    "\n",
    "baseline = df_plot[df_plot[\"prefix\"] == \"1-orign\"].iloc[0]\n",
    "df_plot[\"variation\"] = df_plot.apply(\n",
    "    lambda row: {\n",
    "        col: ((row[col] - baseline[col]) / baseline[col]) * 100\n",
    "        for col in [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"]\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Plotar cada grupo separadamente como barras\n",
    "grouped = df_plot.groupby(\"algorithm\")\n",
    "\n",
    "for algorithm, group in grouped:\n",
    "    group = group.sort_values(\"prefix\")\n",
    "    metrics = [\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"precision\", \"recall\", \"map\"]\n",
    "\n",
    "    # Plotar como barras\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    group[metrics].T.plot(kind=\"bar\", ax=ax, width=0.7)  # Aumenta a largura das barras\n",
    "\n",
    "    # Adicionar os valores e variações acima das barras\n",
    "    for i, prefix in enumerate(group[\"prefix\"]):\n",
    "        for j, metric in enumerate(metrics):\n",
    "            value = group.iloc[i][metric]\n",
    "            variation = group.iloc[i][\"variation\"][metric]\n",
    "            ax.text(\n",
    "                j + i * 0.27 - 0.3, value + 0.005,  # Ajustar posição\n",
    "                f\"{value:.4f} \\n ({variation:.2f}%)\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=8\n",
    "            )\n",
    "\n",
    "    # Personalizar o gráfico\n",
    "    plt.title(f\"Resultados comparativos entre consulta original e fusão com uso do algoritmo {algorithm}\")\n",
    "    plt.xlabel(\"Métricas\")\n",
    "    plt.ylabel(\"Valores\")\n",
    "    plt.legend(title=\"Prefixos\", labels=group[\"prefix\"], loc=\"upper right\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm, group in grouped:\n",
    "    if algorithm == \"bordafuse\":\n",
    "        group = group.sort_values(\"prefix\")\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição das métricas utilizadas:\n",
    "\n",
    "**NDCG** = mede a qualidade dos resultados levando em conta a posição dos itens relevantes. Resultados relevantes em posições mais altas contribuem mais para a pontuação. Pode considerar apenas as k primeiras posições.\n",
    "\n",
    "**Precision** = Fração de instancias relevantes recuperadas, dentre todas as instâncias recuperadas. Responde a pergunta \"Dentre os itens recuperados, quantos são relevantes?\"\n",
    "\n",
    "**Recall** = proporção de itens relevantes retornados em relação ao total de itens relevantes existentes no conjunto de dados. Não considera a posição dos itens, apenas a cobertura. Responde a pergunta \"Dentre todos os itens relevantes, quantos foram recuperados?\"\n",
    "\n",
    "**MAP** = precisão ao longo do ranking ponderando pela posição de cada item relevante. É a média das precisões médias (AP) de várias consultas. Combina precisão e posição, sendo ideal para tarefas onde a ordem dos resultados relevantes é importante.\n",
    "\n",
    "Todas as métricas estão disponíveis em: https://amenra.github.io/ranx/metrics/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
