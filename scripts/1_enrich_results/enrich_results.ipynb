{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich results\n",
    "Incorpora informações úteis ao rankeamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import ast\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from ranx import Qrels\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DADOS ÚTEIS\n",
    "\n",
    "!mkdir -p ../../aditional_data/\n",
    "\n",
    "\n",
    "# ---------------\n",
    "vocabulary_file_source = \"../../aditional_data/word-vocab-small.tsv\"\n",
    "emb_file_source = \"../../aditional_data/wiki-news-300d-1M.vec\"\n",
    "idf_file = '../../aditional_data/idfnew.norm.tsv'\n",
    "original_data = \"../../input_data/samples.pkl\"\n",
    "# ---------------\n",
    "\n",
    "\n",
    "#Carrega o modelo\n",
    "if os.path.isfile(emb_file_source) == False:\n",
    "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "    !unzip wiki-news-300d-1M.vec.zip\n",
    "    !rm wiki-news-300d-1M.vec.zip\n",
    "    !mv ./wiki-news-300d-1M.vec ../../aditional_data/\n",
    "model = KeyedVectors.load_word2vec_format(emb_file_source, encoding=\"utf-8\", binary=False) #FastText model\n",
    "\n",
    "#Carrega o vocabulário\n",
    "if os.path.isfile(vocabulary_file_source) == False:\n",
    "    !wget https://raw.githubusercontent.com/microsoft/MSMARCO-Passage-Ranking/refs/heads/master/Baselines/data/word-vocab-small.tsv -P ../../aditional_data/\n",
    "vocabulary_pd = pd.read_csv(vocabulary_file_source, names=[\"word\", \"id\"], sep=\"\\t\")\n",
    "vocabulary = list(set([str(word).lower() for word in vocabulary_pd[\"word\"].to_list()]))\n",
    "\n",
    "# IDF das palavras - o arquivo contém apenas termos de consultas em conjuntos de treinamento/dev/eval do MSMARCO.\n",
    "# Fonte: https://github.com/microsoft/MSMARCO-Passage-Ranking/tree/master/Baselines\n",
    "if os.path.isfile(idf_file) == False:\n",
    "    !wget https://raw.githubusercontent.com/microsoft/MSMARCO-Passage-Ranking/master/Baselines/data/idfnew.norm.tsv -P ../../aditional_data/\n",
    "\n",
    "\n",
    "qrels_dict = Qrels.from_ir_datasets(\"msmarco-passage/train/judged\").to_dict()\n",
    "data = [\n",
    "    (query_id, passage_id, relevance)\n",
    "    for query_id, passages in qrels_dict.items()\n",
    "    for passage_id, relevance in passages.items()\n",
    "]\n",
    "qrels_df = pd.DataFrame(data, columns=[\"query_idx\", \"passage_idx\", \"relevance\"])\n",
    "\n",
    "# Agrupa julgamentos por id da query julgada para contar número de julgamentos\n",
    "relevant_counts = qrels_df.groupby('query_idx').size().reset_index(name=\"relevant_count\").astype('int64') \n",
    "\n",
    "## Leva até 3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- EMBEDDINGS\n",
    "\n",
    "word_vectors = []\n",
    "new_vocabulary = []\n",
    "for word in vocabulary:\n",
    "    if word in model:\n",
    "        word_vectors.append(np.asarray(model[word], dtype=np.float32))\n",
    "        new_vocabulary.append(word)\n",
    "\n",
    "vocabulary = new_vocabulary.copy()\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "word_vectors.shape\n",
    "\n",
    "## Cada palavra no embedding fasttext tem 300 dimensoes\n",
    "## Agora basta calcular a distancia de cosseno de um vetor para o outro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- FUNÇÕES ÙTEIS\n",
    "\n",
    "def fix_text(text):\n",
    "    text = text.strip()             # Remove espaços iniciais e finais\n",
    "    text = re.sub(' +', ' ', text)  # Remove excesso de espaços\n",
    "    text = text.replace(\" 's\", \"'s\")\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" / \", \"/\")\n",
    "    text = text.replace(\" ?\", \"?\")\n",
    "    return text\n",
    "\n",
    "def calc_spearman(dict1: str, dict2: str, K: int) -> float:\n",
    "    \"\"\"\n",
    "    Examplo de dicionário de input:\n",
    "    dict1 = {'passage1': 1, 'passage2': 2, 'passage3': 3}\n",
    "    dict2 = {'passage1': 3, 'passage2': 2, 'passage3': 1}\n",
    "    \"\"\"\n",
    "\n",
    "    dict1 = ast.literal_eval(dict1)\n",
    "    dict2 = ast.literal_eval(dict2)\n",
    "\n",
    "    common_keys = set(dict1.keys()).intersection(set(dict2.keys()))\n",
    "\n",
    "    common_ranks1 = {key: dict1[key] for key in common_keys}\n",
    "    \n",
    "    top_k_keys = sorted(common_ranks1, key=common_ranks1.get, reverse=True)[:K]\n",
    "\n",
    "    ranks1 = [dict1[key] for key in top_k_keys]\n",
    "    ranks2 = [dict2[key] for key in top_k_keys]\n",
    "\n",
    "    spearman_corr, _ = spearmanr(ranks1, ranks2)\n",
    "\n",
    "    return spearman_corr\n",
    "\n",
    "\n",
    "def calc_avg_precision(dict: str, K: int) -> float:\n",
    "    \"\"\"\n",
    "    Examplo de dicionário de input:\n",
    "    dict = {'passage1': 1, 'passage2': 2, 'passage3': 3}\n",
    "    \"\"\"\n",
    "    \n",
    "    dict = ast.literal_eval(dict)\n",
    "\n",
    "    # Pegar os primeiros K elementos com base no rank (ordem descendente)\n",
    "    top_k_keys = sorted(dict, key=dict.get, reverse=True)[:K]\n",
    "\n",
    "    # Calcular a média de precisão\n",
    "    ranks = [dict[key] for key in top_k_keys]\n",
    "    avg_precision = sum(ranks) / len(ranks) if ranks else 0.0\n",
    "\n",
    "    return avg_precision\n",
    "\n",
    "def similarity(originalPhrase: str, expandedPhrase: str, word_vectors):\n",
    "    '''\n",
    "    Calcula a similaridade entre as palavras diferentes de duas frases.\n",
    "    Se mais de uma palavra for diferente, o resultado será a média das similaridades.\n",
    "    '''\n",
    "    similarities = []\n",
    "    originalPhraseWords = originalPhrase.split()\n",
    "    expandedPhraseWords = expandedPhrase.split()\n",
    "    \n",
    "    for wordIndex in range(0, len(originalPhraseWords)):\n",
    "        originalWord = originalPhraseWords[wordIndex]\n",
    "        expandedWord = expandedPhraseWords[wordIndex]\n",
    "\n",
    "        if(originalWord == expandedWord):\n",
    "            continue\n",
    "\n",
    "        emb1 = word_vectors[vocabulary.index(originalWord)]\n",
    "        emb2 = word_vectors[vocabulary.index(expandedWord)]\n",
    "\n",
    "        similarities.extend(cosine_similarity([emb1], [emb2]))\n",
    "     \n",
    "    return(sum(similarities)/len(similarities))\n",
    "\n",
    "def idf(originalPhrase: str, expandedPhrase: str, idfListOriginal: list, idfListExpanded: list):\n",
    "    '''\n",
    "        Recupera o idf das palavras diferentes de duas frases, além da diferença entre o idf da frase original e da expandida.\n",
    "        Se mais de uma palavra for diferente, o resultado será a média dos idfs.\n",
    "    '''    \n",
    "\n",
    "    if len(idfListOriginal) != len(originalPhrase.split()) or len(idfListExpanded) != len(expandedPhrase.split()):\n",
    "        raise ValueError(\"idfListOriginal and idfListExpanded must be the same length as originalPhrase and expandedPhrase\")\n",
    "\n",
    "    \n",
    "    expanded_idfs = []\n",
    "    difference_idfs = []\n",
    "\n",
    "    originalPhraseWords = originalPhrase.split()\n",
    "    expandedPhraseWords = expandedPhrase.split()\n",
    "\n",
    "    for wordIndex in range(0, len(originalPhraseWords)):\n",
    "        originalWord = originalPhraseWords[wordIndex]\n",
    "        expandedWord = expandedPhraseWords[wordIndex]\n",
    "\n",
    "        if(originalWord == expandedWord):\n",
    "            continue\n",
    "\n",
    "\n",
    "        if idfListOriginal[wordIndex] is None or idfListExpanded[wordIndex] is None:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "        \n",
    "        expanded_idfs.append(idfListExpanded[wordIndex])\n",
    "        difference_idfs.append(idfListOriginal[wordIndex]-idfListExpanded[wordIndex])\n",
    "        \n",
    "\n",
    "    mean_expanded_idfs = sum(expanded_idfs)/len(expanded_idfs)\n",
    "    mean_of_diference_idfs = sum(difference_idfs)/len(difference_idfs)\n",
    "    \n",
    "    return mean_expanded_idfs, mean_of_diference_idfs\n",
    "\n",
    "# Função para tokenizar documentos\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Função para retornar o vetor TF-IDF de uma sentença\n",
    "def get_sentence_tfidf(sentence, idf):\n",
    "    tokenized_sentence = tokenize(sentence)\n",
    "    sentence_tf = Counter(tokenized_sentence)\n",
    "    sentence_tfidf = {term: (sentence_tf[term] / sum(sentence_tf.values())) * idf.get(term, 0) for term in tokenized_sentence}\n",
    "    return sentence_tfidf\n",
    "\n",
    "# Função para retornar o IDF de cada termo em uma sentença\n",
    "def get_sentence_idf(sentence, idf):\n",
    "    tokenized_sentence = tokenize(sentence)\n",
    "    sentence_idf = {term: idf.get(term, 0) for term in tokenized_sentence}\n",
    "    return sentence_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recupera as informações textuais e identificadores originais das queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse arquivo contém as expansões antes do rankeamento (feito no DLR)\n",
    "expanded_queries_info_pd = pd.DataFrame(pd.read_pickle(original_data))\n",
    "\n",
    "expanded_queries_info_pd['query_idx'] = expanded_queries_info_pd['query_exp_id'].apply(lambda query: query.split('_')[0]).astype('int64')\n",
    "expanded_queries_info_pd['exp_num'] = expanded_queries_info_pd['query_exp_id'].apply(lambda query: query.split('_')[1]).astype('int64')\n",
    "\n",
    "expanded_queries_info_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recupera o rankeamento das queries expandidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O rankeamento feito na DLR gera diversos folds. É preciso escolher um deles para usar no estudo\n",
    "fold = 0\n",
    "\n",
    "expanded_queries_rank_pd =  pd.read_csv(f\"../../input_data/RetrieverBERT_queries_train_judged_expanded_{fold}.rnk.csv\", sep=\"\\t\")     \n",
    "expanded_queries_rank_pd = expanded_queries_rank_pd.sort_values(by=['Query']).drop_duplicates()\n",
    "\n",
    "print(f\"Foram obtidos {len(expanded_queries_rank_pd)} resgistros dos folds. Cada um referente a um par (expansão-rank).\")\n",
    "expanded_queries_rank_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa o número referente ao par\n",
    "expanded_queries_rank_pd['query_run_idx'] = expanded_queries_rank_pd['Query'].apply(lambda query: query.split('_')[1]).astype('int64')  # transformação: query_3 -> 3\n",
    "expanded_queries_docs_rank_pd = expanded_queries_rank_pd[[\"query_run_idx\", \"Passage_Scores\"]]\n",
    "\n",
    "expanded_queries_docs_rank_pd.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera o identificador original das queries\n",
    "expanded_queries_docs_rank_info_pd = expanded_queries_docs_rank_pd.merge(expanded_queries_info_pd, left_on='query_run_idx', right_on='idx', suffixes=('_x', ''))\n",
    "expanded_queries_docs_rank_info_pd = expanded_queries_docs_rank_info_pd[['query_idx', 'exp_num','Passage_Scores', 'query_exp_id']]\n",
    "\n",
    "print(f\"São {expanded_queries_docs_rank_info_pd['query_idx'].nunique()} queries originais sendo utilizadas aqui.\")\n",
    "expanded_queries_docs_rank_info_pd.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifica as queries que deram origem às expansões (aquelas com 'exp_num' igual a 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_expansions = expanded_queries_docs_rank_info_pd.groupby('query_idx')\n",
    "\n",
    "one_expansions = []\n",
    "\n",
    "for query_idx, group in grouped_expansions:\n",
    "    filtered_rows = group[group['exp_num'] == 1]\n",
    "    one_expansions.append(filtered_rows)\n",
    "\n",
    "one_expansions = pd.concat(one_expansions, ignore_index=True)\n",
    "one_expansions = one_expansions.drop_duplicates()\n",
    "\n",
    "#print(len(one_expansions))\n",
    "one_expansions.head(15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega o número de documentos relevantes por query (obtidos da MSMARCO)\n",
    "original_queries = one_expansions.merge(relevant_counts, on=\"query_idx\")\n",
    "original_queries.rename(columns={\"Passage_Scores\":\"original_passage_scores\"}, inplace=True)\n",
    "\n",
    "original_queries.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifica as queries são realmente expansões (aquelas com 'exp_num' diferente de 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries = expanded_queries_docs_rank_info_pd[expanded_queries_docs_rank_info_pd[\"exp_num\"] != 1].copy()\n",
    "expanded_queries.rename(columns={\"Passage_Scores\":\"expansion_passage_scores\"}, inplace=True)\n",
    "\n",
    "expanded_queries.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INÍCIO DO ENRIQUECIMENTO COM INFORMAÇÕES ADICIONAIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unifica as queries originais com as expansões\n",
    "E inclui o número de documentos relevantes julgados na MsMarco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries_ranks_pd = pd.merge(left=expanded_queries, right=original_queries, on='query_idx')\n",
    "\n",
    "#expanded_queries_ranks_pd[\"original_passage_scores\"] = expanded_queries_ranks_pd.apply(\n",
    "#    lambda row: original_queries.loc[original_queries[\"query_idx\"] == row[\"query_idx\"]][\"original_passage_scores\"].values[0], axis=1\n",
    "#)\n",
    "\n",
    "#expanded_queries_ranks_pd[\"relevant_count\"] = expanded_queries_ranks_pd.apply(\n",
    "#    lambda row: original_queries.loc[original_queries[\"query_idx\"] == row[\"query_idx\"]][\"relevant_count\"].values[0], axis=1\n",
    "#)\n",
    "expanded_queries_ranks_pd. head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcula o Spearman entre cada registro original e suas expansões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd = expanded_queries_ranks_pd.copy()\n",
    "spearman_pd['spearman'] = spearman_pd.apply(\n",
    "    lambda row: calc_spearman(row['original_passage_scores'], row['expansion_passage_scores'], row['relevant_count']),\n",
    "    axis=1   \n",
    ") \n",
    "\n",
    "print(len(spearman_pd))\n",
    "spearman_pd.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcula a média dos valores de relevância da query original e das expansões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd['avg_precision_query_original'] = spearman_pd.apply(\n",
    "    lambda row: calc_avg_precision(row['original_passage_scores'], row['relevant_count']),\n",
    "    axis=1   \n",
    ")\n",
    "\n",
    "spearman_pd['avg_precision_query_expansao'] = spearman_pd.apply(\n",
    "    lambda row: calc_avg_precision(row['expansion_passage_scores'], row['relevant_count']),\n",
    "    axis=1   \n",
    ")\n",
    "\n",
    "print(len(spearman_pd))\n",
    "spearman_pd.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtém os textos das queries original e expandida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spearman_pd[\"query_original\"] = spearman_pd.apply(\n",
    "    lambda row: expanded_queries_info_pd.loc[expanded_queries_info_pd[\"query_idx\"] == row[\"query_idx\"], \"query\"].values[0], axis=1\n",
    ")\n",
    "\n",
    "spearman_pd[\"query_expandida\"] = spearman_pd.apply(\n",
    "    lambda row: expanded_queries_info_pd.loc[\n",
    "        (expanded_queries_info_pd[\"query_idx\"] == row[\"query_idx\"]) & \n",
    "        (expanded_queries_info_pd[\"exp_num\"] == row[\"exp_num_x\"]), \n",
    "        \"query\"\n",
    "    ].values[0], axis=1\n",
    ")\n",
    "\n",
    "print(len(spearman_pd))\n",
    "spearman_pd.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTRA queries sem julgamento suficiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não é mais necessario, já que essa filtragem foi feita antes de realizar a expansão\n",
    "\n",
    "#filtered_df = spearman_pd[spearman_pd['relevant_count'] >= 5]\n",
    "#filtered_df.count()\n",
    "#len(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agrega IDF das palavras da query original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_pd = pd.read_csv(\"../../aditional_data/idfnew.norm.tsv\", sep=\"\\t\", header=None, names=[\"word\", \"idf\"])\n",
    "\n",
    "idf_dict = dict(zip(IDF_pd['word'], IDF_pd['idf']))\n",
    "\n",
    "#Mapeia palavras e seus valores de IDF\n",
    "def map_idf(query):\n",
    "    words = query.split()\n",
    "    idf_values = [idf_dict.get(word, None) for word in words]  # Use None if the word is not found\n",
    "    return idf_values\n",
    "\n",
    "spearman_pd['idf_original_values'] = spearman_pd['query_original'].apply(map_idf)\n",
    "spearman_pd['idf_expanded_values'] = spearman_pd['query_expandida'].apply(map_idf)\n",
    "\n",
    "spearman_pd.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria o indicador de similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd[\"words_similarity\"] = spearman_pd.apply(lambda row: similarity(row[\"query_original\"], row[\"query_expandida\"], word_vectors)[0], axis=1)\n",
    "\n",
    "spearman_pd.head(4)\n",
    "# Demora cerca de 2m para executar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria os indicadores de idf (idf do(s) termo(s) expandido(s) e diferença entre idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd[[\"expansion_idf\", \"expansion_idf_difference\"]] = spearman_pd.apply(\n",
    "    lambda row: idf(row[\"query_original\"], row[\"query_expandida\"], row[\"idf_original_values\"], row[\"idf_expanded_values\"]), \n",
    "    axis=1,\n",
    "    result_type=\"expand\"\n",
    "    )\n",
    "\n",
    "spearman_pd.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria os labels com base na precisão média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se avg_precision_query_expansao >= avg_precision_query_original então label = 1\n",
    "spearman_pd[\"label\"] = spearman_pd.apply(\n",
    "    lambda row: 1 if row[\"avg_precision_query_expansao\"] >= row[\"avg_precision_query_original\"] else 0, axis=1\n",
    "    )\n",
    "\n",
    "spearman_pd.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salva resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_pd = spearman_pd.rename(columns={'exp_num_x':'exp_num'})\n",
    "spearman_pd.to_csv(f'./queries_train_judged_expanded_enriched.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
