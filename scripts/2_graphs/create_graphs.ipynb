{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria grafos de palavras expandidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_palavras_distintas(nodes):\n",
    "    original_phrase = None\n",
    "    original_idf = None\n",
    "\n",
    "    for phrase, attributes in nodes:\n",
    "        if attributes['type'] == 'original':\n",
    "            original_phrase = phrase\n",
    "            original_idf = ast.literal_eval(attributes['idf'])\n",
    "            break\n",
    "\n",
    "    original_words = original_phrase.split()\n",
    "    resultado = \"\"\n",
    "    palavras_identificadas = set()\n",
    "\n",
    "    # Encontra as palavras diferentes da frase original nas frases expandidas\n",
    "    for phrase, attributes in nodes:\n",
    "        if attributes['type'] == 'expanded':\n",
    "            expanded_words = phrase.split()\n",
    "            # Identifica as palavras da frase original que não estão na frase expandida\n",
    "            palavras_distintas = set(original_words).difference(expanded_words)\n",
    "\n",
    "            # Constrói a string de resultado com as palavras distintas e seus IDFs\n",
    "            novas_palavras = palavras_distintas.difference(palavras_identificadas)\n",
    "\n",
    "            if novas_palavras:\n",
    "                for palavra in novas_palavras:\n",
    "                    idx = original_words.index(palavra)\n",
    "                    resultado += f\"IDF da palavra '{palavra}': {original_idf[idx]:.4f}\\n\"\n",
    "                # Adiciona as palavras novas ao conjunto de palavras identificadas\n",
    "                palavras_identificadas.update(novas_palavras)\n",
    "\n",
    "    return resultado\n",
    "\n",
    "def format_label(query_original, query_expandida, precision):\n",
    "    original_words = set(query_original.split())\n",
    "    expanded_words = query_expandida.split()\n",
    "    formatted_words = [\n",
    "        r\"$\\mathbf{\" + word + \"}$\" if word not in original_words else word\n",
    "        for word in expanded_words\n",
    "    ]\n",
    "    formatted_text = \" \".join(formatted_words)\n",
    "    return f\"{formatted_text}\\n({precision:.3f})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recupera o dataset com as queries, expansões e semelhança entre elas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.read_csv(\"../1_enrich_results/queries_train_judged_expanded_enriched.csv\", sep=\"\\t\")\n",
    "queries_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "len(queries_df)\n",
    "queries_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantém apenas registros com um número mínimo de documentos julgados\n",
    "filtered_df = queries_df[queries_df['relevant_count'] >= 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agrupa os registros original e expandidos de uma mesma query\n",
    "grouped_data = filtered_df.groupby('query_idx')\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plota o grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 4  #   <------------------------------------- ESCOLHA AQUI O NÚMERO DE PLOTS A SEREM GERADOS\n",
    "pdf_path = 'expansion_plots.pdf' # os plots serão salvos aqui\n",
    "pdf = PdfPages(pdf_path)\n",
    "\n",
    "# Itera sobre grupos de linhas provenientes da mesma query original ('idx_query_original')\n",
    "count_plots = 1\n",
    "for idx_query_original, group in grouped_data:\n",
    "    data = group\n",
    "\n",
    "    data['expanded_words'] = data.apply(lambda row: [(word, row['query_expandida'].split().index(word)) for word in row['query_expandida'].split() if word not in set(row['query_original'].split())], axis=1)\n",
    "\n",
    "    edges = []\n",
    "    for _, row in data.iterrows():\n",
    "        if np.isnan(row['spearman']):\n",
    "            continue\n",
    "\n",
    "        edges.append((row['query_original'], row['query_expandida'], {\n",
    "            'spearman': row['spearman'],\n",
    "            'avg_precision_query_original': row['avg_precision_query_original'],\n",
    "            'avg_precision_query_expansao': row['avg_precision_query_expansao'],\n",
    "            'idf_original_words':row['idf_original_values'],\n",
    "            'num_passagens':row['relevant_count']\n",
    "        }))\n",
    "\n",
    "    if not edges:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    #Create the graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_node(edge[0], avg_precision=edge[2]['avg_precision_query_original'], type='original', idf=edge[2]['idf_original_words'], num_passagens=edge[2]['num_passagens'])\n",
    "        G.add_node(edge[1], avg_precision=edge[2]['avg_precision_query_expansao'], type='expanded')\n",
    "        G.add_edge(edge[0], edge[1], spearman=edge[2]['spearman'])\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    original_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'original']\n",
    "    expanded_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'expanded']\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=original_nodes, node_color='blue', node_size=700, label='Original')\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=expanded_nodes, node_color='green', node_size=700, label='Expanded')\n",
    "\n",
    "    edge_colors = [G[u][v]['spearman'] for u, v in G.edges()]\n",
    "    if edge_colors:\n",
    "        range_edge_colors = max(edge_colors) - min(edge_colors)\n",
    "        if range_edge_colors == 0:\n",
    "            edge_colors_normalized = [0.5 for _ in edge_colors]\n",
    "        else:\n",
    "            edge_colors_normalized = [(value - min(edge_colors)) / range_edge_colors for value in edge_colors]\n",
    "\n",
    "        cmap = plt.cm.viridis\n",
    "        colors = [cmap(color) for color in edge_colors_normalized]\n",
    "\n",
    "        nx.draw_networkx_edges(G, pos, edge_color=colors, width=2)\n",
    "\n",
    "        edge_labels = {(u, v): f\"{G[u][v]['spearman']:.3f}\" for u, v in G.edges()}\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(edge_colors), vmax=max(edge_colors)))\n",
    "        sm.set_array([])\n",
    "        plt.colorbar(sm, ax=ax, label='Correlação Spearman')\n",
    "\n",
    "    original_labels = {node: f\"{node}\\n({attr['avg_precision']:.3f})\" for node, attr in G.nodes(data=True) if attr['type'] == 'original'}\n",
    "    expanded_labels = {node: format_label(edge[0], node, attr['avg_precision']) for node, attr in G.nodes(data=True) if attr['type'] == 'expanded'}\n",
    "\n",
    "    def adjust_labels(pos, x_shift=0, y_shift=0.05):\n",
    "        return {node: (coord[0] + x_shift, coord[1] + y_shift) for node, coord in pos.items()}\n",
    "\n",
    "    label_pos = adjust_labels(pos, y_shift=0.13)\n",
    "\n",
    "    nx.draw_networkx_labels(G, label_pos, labels=original_labels, font_color='black')\n",
    "    nx.draw_networkx_labels(G, label_pos, labels=expanded_labels, font_color='black')\n",
    "\n",
    "    # Recupera o número de julgamentos (e de passagens) utilizados\n",
    "    num_passagens_text = \"\"\n",
    "    for node, attr in G.nodes(data=True):\n",
    "        if attr['type'] == 'original':\n",
    "            num_passagens_text = f\"K: {attr['num_passagens']}\\n\"\n",
    "\n",
    "    # Calcula o IDF das palavras expandidas\n",
    "    idf_original_words = encontrar_palavras_distintas(G.nodes(data=True))\n",
    "\n",
    "    box_text = \" INFORMAÇÕES ADICIONAIS \\n\" + num_passagens_text + idf_original_words\n",
    "    plt.gcf().text(0.132, 0.14, box_text, fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    plt.title(f\"Performance e correlação da expansão da query {idx_query_original}\")\n",
    "\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Query Original'),\n",
    "               plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=8, label='Query Expandida')]\n",
    "\n",
    "    plt.legend(handles=handles, loc='upper right')\n",
    "\n",
    "    plt.xlim(-1.8, 1.8) #plt.xlim(-2.5, 2.5)\n",
    "    plt.ylim(-1.8, 1.8) #plt.ylim(-2.5, 2.5)\n",
    "\n",
    "    pdf.savefig()\n",
    "    if count_plots >= num_plots:\n",
    "        break\n",
    "    else:\n",
    "        count_plots += 1\n",
    "\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
