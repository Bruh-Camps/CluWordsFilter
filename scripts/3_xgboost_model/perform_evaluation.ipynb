{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega dataset com as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset = pd.read_csv(\"../1_enrich_results/queries_train_judged_expanded_enriched.csv\", sep='\\t')\n",
    "clean_dataset = clean_dataset[[\"label\", \"relevant_count\",\"spearman\" , \"words_similarity\",  \"expansion_idf\", \"expansion_idf_difference\"]] \n",
    "clean_dataset = clean_dataset.rename(columns={\n",
    "                                                'words_similarity': 'words_semantic_similarity',\n",
    "                                                'relevant_count': 'k_relevance_judgments',\n",
    "                                                'spearman':'spearman_rank_correlation'\n",
    "                                            })\n",
    "\n",
    "clean_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faz o k-fold evaluation usando XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "auc_scores = []\n",
    "f1_micro_scores = []\n",
    "f1_macro_scores = []\n",
    "\n",
    "cumulative_cm = np.zeros((2, 2))\n",
    "\n",
    "all_true_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "fig, axs = plt.subplots(1, n_folds, figsize=(20, 5))\n",
    "fig.suptitle(\"Matriz de confusão para cada fold\")\n",
    "\n",
    "# percorre os folds\n",
    "last_precison_score = 0\n",
    "for i, (train_index, test_index) in enumerate(kf.split(clean_dataset)):\n",
    "    X_train, X_test = clean_dataset.drop('label', axis=1).iloc[train_index], clean_dataset.drop('label', axis=1).iloc[test_index]\n",
    "    y_train, y_test = clean_dataset['label'].iloc[train_index], clean_dataset['label'].iloc[test_index]\n",
    "    print(f\"Fold {i} -> Tamanho X_train: {len(X_train)}, Tamanho X_test: {len(X_test)}, Tamanho y_train: {len(y_train)}, Tamanho y_test: {len(y_test)}\")\n",
    "    \n",
    "    initial_model = xgb.XGBClassifier(\n",
    "                                        eval_metric='logloss', \n",
    "                                        n_estimators=500, \n",
    "                                        max_depth=5,\n",
    "                                        learning_rate=0.5\n",
    "                                    )\n",
    "    initial_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = initial_model.predict(X_test)\n",
    "    y_pred_proba = initial_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    all_true_labels.extend(y_test)\n",
    "    all_predictions.extend(y_pred)\n",
    "    \n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_pred_proba))\n",
    "    f1_micro_scores.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    f1_macro_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axs[i])\n",
    "    axs[i].set_title(f'Fold {i+1}')\n",
    "    axs[i].set_xlabel('Predição do modelo')\n",
    "    axs[i].set_ylabel('Label real')\n",
    "    \n",
    "    cumulative_cm += cm\n",
    "\n",
    "    # Salvar o modelo caso a precisão tenha sido maior que a anteior\n",
    "    if precision_score(y_test, y_pred) >= last_precison_score:\n",
    "        initial_model.save_model(f\"xgboost_model.json\")\n",
    "        print(f\"modelo {i} salvo\")\n",
    "\n",
    "    last_precison_score = precision_score(y_test, y_pred)\n",
    "\n",
    "#Calcula a matriz de confusão\n",
    "mean_cm = cumulative_cm / n_folds\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mean_cm, annot=True, fmt=\".2f\", cmap='Blues')\n",
    "plt.title(\"Média da matriz de confusão para todos os folds\")\n",
    "plt.xlabel(\"Predição do modelo\")\n",
    "plt.ylabel(\"Rótulo real\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Acurácia: {sum(accuracy_scores) / n_folds}\")\n",
    "print(f\"Precisão: {sum(precision_scores) / n_folds}\")\n",
    "print(f\"Recall: {sum(recall_scores) / n_folds}\")\n",
    "#print(f\"AUC: {sum(auc_scores) / n_folds}\")\n",
    "print(f\"F1 Micro: {sum(f1_micro_scores) / n_folds}\")\n",
    "print(f\"F1 Macro: {sum(f1_macro_scores) / n_folds}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = permutation_importance(initial_model, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "sorted_idx = np.argsort(results.importances_mean)\n",
    "\n",
    "plt.barh([X_test.columns[i] for i in sorted_idx], [results.importances_mean[i] for i in sorted_idx], height=0.4)\n",
    "plt.xlabel('Contribuição de cada feature para o modelo')\n",
    "plt.title('Permutação de features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevância de cada métrica +/- desvio padrão\n",
    "for i in results.importances_mean.argsort()[::-1]:\n",
    "     if results.importances_mean[i] - 2 * results.importances_std[i] > 0:\n",
    "         print(f\"{initial_model.feature_names_in_[i]:<8}\"\n",
    "               f\"\\t\\t {results.importances_mean[i]:.3f}\"\n",
    "               f\" +/- {results.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação de falsos positivos e negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte os rótulos e previsões em um dataframe de análise\n",
    "results_df = pd.DataFrame({\n",
    "    'True Labels': all_true_labels,\n",
    "    'Predictions': all_predictions\n",
    "})\n",
    "\n",
    "# Identifica as categorias\n",
    "false_positives = results_df[(results_df['True Labels'] == 0) & (results_df['Predictions'] == 1)]\n",
    "false_negatives = results_df[(results_df['True Labels'] == 1) & (results_df['Predictions'] == 0)]\n",
    "true_positives = results_df[(results_df['True Labels'] == 1) & (results_df['Predictions'] == 1)]\n",
    "true_negatives = results_df[(results_df['True Labels'] == 0) & (results_df['Predictions'] == 0)]\n",
    "\n",
    "# Combina com o dataset original para recuperar as features\n",
    "X_full = clean_dataset.drop('label', axis=1) \n",
    "\n",
    "false_positive_indices = false_positives.index\n",
    "fp_feature_values = X_full.iloc[false_positive_indices]\n",
    "false_negative_indices = false_negatives.index\n",
    "fn_feature_values = X_full.iloc[false_negative_indices]\n",
    "true_positive_indices = true_positives.index\n",
    "tp_feature_values = X_full.iloc[true_positive_indices]\n",
    "true_negative_indices = true_negatives.index\n",
    "tn_feature_values = X_full.iloc[true_negative_indices]\n",
    "\n",
    "# Plota as comparações\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "tp_feature_values.boxplot(ax=axs[0, 0], rot=45)\n",
    "axs[0, 0].set_title('Verdadeiros positivos')\n",
    "\n",
    "fp_feature_values.boxplot(ax=axs[0, 1], rot=45)\n",
    "axs[0, 1].set_title('Falsos positivos')\n",
    "\n",
    "fn_feature_values.boxplot(ax=axs[1, 0], rot=45)\n",
    "axs[1, 0].set_title('Falsos negativos')\n",
    "\n",
    "tn_feature_values.boxplot(ax=axs[1, 1], rot=45)\n",
    "axs[1, 1].set_title('Verdadeiros Negativos')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipóteses de teste para falsos positivos\n",
    "- Similaridade não é boa o suficiente\n",
    "- A palavra substituída é genérica demais\n",
    "- O idf dela é alto demais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "tp_feature_values.boxplot(column=\"words_semantic_similarity\", ax=ax, positions=[1], widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"skyblue\"))\n",
    "fp_feature_values.boxplot(column=\"words_semantic_similarity\", ax=ax, positions=[2], widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"salmon\"))\n",
    "\n",
    "ax.set_title('Comparação da similaridade semântica entre verdadeiros positivos e falsos positivos')\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Verdadeiros positivos', 'Falsos positivos'])\n",
    "\n",
    "ax.set_ylim(0.6, 1.0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "tp_feature_values.boxplot(column=\"spearman_rank_correlation\", ax=ax, positions=[1], widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"skyblue\"))\n",
    "fp_feature_values.boxplot(column=\"spearman_rank_correlation\", ax=ax, positions=[2], widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"salmon\"))\n",
    "\n",
    "ax.set_title('Comparação da correlação Spearman entre verdadeiros positivos e falsos positivos')\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Verdadeiros positivos', 'Falsos positivos'])\n",
    "\n",
    "# ax.set_ylim(0.6, 1.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "tp_feature_values.boxplot(column=\"expansion_idf\", ax=ax, positions=[1], widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"skyblue\"))\n",
    "fp_feature_values.boxplot(column=\"expansion_idf\", ax=ax, positions=[2], widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"salmon\"))\n",
    "\n",
    "ax.set_title('Comparação de idf da palavra expandida entre verdadeiros positivos e falsos positivos')\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Verdadeiros positivos', 'Falsos positivos'])\n",
    "\n",
    "# ax.set_ylim(0.6, 1.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
